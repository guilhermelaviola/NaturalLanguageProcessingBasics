{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPE2qJqd4dZOoNMxG8hOYFm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guilhermelaviola/NaturalLanguageProcessingBasics/blob/main/NextWordPredictionModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install keras.layers.core"
      ],
      "metadata": {
        "id": "QcUtLw-4J85j",
        "outputId": "51b623c2-e6ea-4eda-fdc8-45547112ae16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement keras.layers.core (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for keras.layers.core\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mAgW_vFHJiu"
      },
      "outputs": [],
      "source": [
        "# Importing all the necessary libraries:\n",
        "import numpy as np\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import LSTM\n",
        "#from keras.layers.core import Dense, Activation\n",
        "from keras.optimizers import RMSprop\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import heapq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the dataset:\n",
        "path = '1661-0.txt'\n",
        "text = open(path).read().lower()\n",
        "print('corpus length:', len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "JXLXRTRfJNl2",
        "outputId": "95f8adb6-f726-411c-9e8d-cabaa91b8221"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-1b044f83cdb0>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'1661-0.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'corpus length:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '1661-0.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the dataset into each word in order but without the presence\n",
        "# of some special characters:\n",
        "tokenizers = RegexpTokenizer(r'w+')\n",
        "words = tokenizer.tokenize(text)"
      ],
      "metadata": {
        "id": "o-ozNWOcJwbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Requiring a dictionary with each word in the data within the list\n",
        "# of unique words as the key, and its significant portions as values:\n",
        "unique_words = np.unique(words)\n",
        "#unique_word_index = dict((c, i) for i, c in enumerate(unique_words))"
      ],
      "metadata": {
        "id": "fTabnqCZKH_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining Word length which will represent the number of previous words that\n",
        "# will determine our next word.\n",
        "WORD_LENGTH = 5\n",
        "previous_words = []\n",
        "next_words = []\n",
        "for i in range(len(words) - WORD_LENGTH):\n",
        "  previous_words.append(words[i : i + WORD_LENGTH])\n",
        "  next_words.append(words[i + WORD_LENGTH])\n",
        "print(previous_words[o])\n",
        "print(next_words[0])"
      ],
      "metadata": {
        "id": "5RZx-g4ELerG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating numpy arrays x for storing the features and y for\n",
        "# storing its corresponding label.\n",
        "X = np.zeros((len(previous_words), WORD_LENGTH, len(unique_words)), dtype = bool)\n",
        "Y = np.zeros((len(next_words), len(unique_words)), dtype = bool)\n",
        "for i, each_word in enumerate(each_words):\n",
        "  X[i, j, unique_word_index[each_word]] = 1\n",
        "  Y[i, unique_word_index[next_words[i]]] = 1"
      ],
      "metadata": {
        "id": "CXOYCza_LOoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the sequence of words:\n",
        "print(X[0][0])"
      ],
      "metadata": {
        "id": "1HLgFFDVMdXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the LSTM model, which is a Recurrent Neural networks for\n",
        "# next word prediction model:\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape = (WORD_LENGTH, len(unique_words))))\n",
        "model.add(Dense(len(unique_words)))\n",
        "model.add(Activation('softmax'))"
      ],
      "metadata": {
        "id": "TfNefMs7MokT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the word prediction with 20 epochs:\n",
        "optimizer = RMSprop(lr = 0.01)\n",
        "model.compile(loss = 'categorical_crossentropy',\n",
        "              optimizer = optimizer,\n",
        "              metrics = ['accuracy'])\n",
        "history = model.fit(X,\n",
        "                    Y,\n",
        "                    validation_split = 0.05,\n",
        "                    batch_size = 128,\n",
        "                    epochs = 2,\n",
        "                    shuffle = True).history"
      ],
      "metadata": {
        "id": "zsCFqJAgQhTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the model for future use:\n",
        "model.save('keras_next_word_model.h5')\n",
        "pickle.dump(history, open('history.p', 'wb'))\n",
        "model = load_model('keras_next_word_model.h5')\n",
        "history = picke.load(open('history.p', 'rb'))"
      ],
      "metadata": {
        "id": "Oc6Mwihie1c3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the model based on how its accuracy changes while training:\n",
        "plt.plot(history['acc'])\n",
        "plt.plot(history['val_acc'])\n",
        "plt.title('model_accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc = 'upper left')"
      ],
      "metadata": {
        "id": "QSIFNCilpaws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the model based on how its loss changes while training:\n",
        "plt.plot(history['loss'])\n",
        "plt.plot(history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc = 'upper left')"
      ],
      "metadata": {
        "id": "VDtaAB7-qKBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building a program to predict th next word using the training model:\n",
        "def prepare_input(text):\n",
        "  x = np.zeros((1, SEQUENCE_LENGTH, len(chars)))\n",
        "  for t, char in enumerate(text):\n",
        "    x[o, t, char_indexes[char]] = 1.\n",
        "  return x"
      ],
      "metadata": {
        "id": "nYO8sfkoqCJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the fnction (making sure to use a lower() function while giving input )\n",
        "prepare_input('This is an example of input'.lower())"
      ],
      "metadata": {
        "id": "xY9w908bvEAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking if the created function is working:\n",
        "def prepare_input(text):\n",
        "  x = np.zeros((1, WORD_LENGTH, len(unique_words)))\n",
        "  for t, word in enumerate(text.split()):\n",
        "    print(word)\n",
        "    x[0, t, unique_word_index[word]] = 1\n",
        "  return x\n",
        "prepare_input('It is not a lack'.lower())"
      ],
      "metadata": {
        "id": "CqbhpT8PviVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a function to return samples:\n",
        "def sample(preds, top_n = 3):\n",
        "  preds = np.asarray(preds).astype('float64')\n",
        "  preds = np.log(preds)\n",
        "  exp_preds = np.expp(preds)\n",
        "  preds = exp_preds / np.sum(exp_preds)\n",
        "\n",
        "  return heapq.nlargest(top_n, range(len(preds), preds.take)"
      ],
      "metadata": {
        "id": "no8d_6_NwVPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_completion(text):\n",
        "  original_text = text\n",
        "  generated = text\n",
        "  generated = text\n",
        "  compeltion = ''\n",
        "  while True:\n",
        "    X = prepare_input(text)\n",
        "    preds = model.predict(X, verbose = 0)[0]\n",
        "    next_index = sample(preds. top_n = 1)[0]\n",
        "    next_char = indexes_char[next_index]\n",
        "    text = text[1:] + next_char\n",
        "    completion += next_char\n",
        "\n",
        "    if len(original_text + completion) + 2 > len(original_text)\n",
        "    and next_char == ' ':\n",
        "      return completion"
      ],
      "metadata": {
        "id": "zwPVG_VYxk57"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}